\documentclass{amsart}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{ifpdf}
\usepackage{url}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsfonts}
\title{Project 2 - Nearest Neighbors Classification and Regression}
\author{David Atlas}
\begin{document}
    \begin{abstract}
        This paper will introduce the K Nearest-Neighbors (KNN) algorithm for supervised learning.
        It will also introduce two variants - Edited KNN and Condensed KNN - that provide
        more efficient predictions (and potentially better performance). In several experiments on
        real world classification problems, the algorithms perform comparably. On regression
        problems, KNN does not perform particularly well compared to an OLS baseline.
    \end{abstract}
    \maketitle
    \section{Problem Statement \& Hypothesis}
    A non-parametric density estimation technique, K-Nearest Neighbors,
    is adapted for supervised learning, and applied to
    real world classification and regression problems. Additionally,
    two variants are introduced that seek to make prediction more efficient
    by reducing the set of points included in calculated the nearest neighbors. These variants
    are only valid for classifications problems, and so are not used on regression problems.

    Because these algorithms rely on density estimation, it is expected that they will
    not perform well in high dimensional spaces. Additionally, the algorithms cannot detect
    interactions between features, and so would not be expected to perform well if there are
    interactions. The density estimation technique (explained in more detail below) treats
    all features equally, and so performance may be poor if many unnecessary features are
    included in the model, as the important features may be drowned out by the distances
    of the unimportant features.

    Out of the algorithms in this paper, the standard KNN should outperform in situations
    where the data is not too noisy, and the classes are reasonably seperated. An Edited KNN
    model might perform better on noisy data, as points that lie in the space of a different
    class are deleted from the prediction set. Additionally, if there are irrelevant
    features included in the algorithm, an Edited KNN may perform well, as it may drop points in
    a space that are far from the true class center only along irrelevant attribute dimensions.

    A Condensed KNN might perform poorly on noisy
    data, as points far from their class center will be added back into the prediction set.
    However, it might do better in a situation where classes occupy multiple spaces.

    \section{Description of Algorithms}
    \subsection*{K Nearest Neighbors}
    The K Nearest-Neighbors algorithm relies on the general hypothesis that points
    that are near each other in feature space are also near each other in hypothesis space.
    As such, for a given instance, the prediction class is chosen via a majority
    voting scheme within the $k$ training points that are nearest to the instance.

    The K Nearest-Neighbors algorithm is a lazy algorithm, in that it does not
    do anything on fitting, but does all the work at prediction time. It calculates the
    distance between the instance to label, $x_0$, and the training instances, $X = (x_1, \cdots, x_n)$.
    The training points with the $k$ shortest distances are then selected, and the class with the
    most elements of the $k$ is chosen for $x_0$. Ties are broken randomly.

    There are two choices to make regarding the algorithm. One is the choice of $k$, or the number
    of neighbors that participate in the majority vote. This value can be chosen via
    a holdout validation set. The other choice is that of the distance metric to be used in
    finding the "nearest" neighbors. This is commonly the 2-norm (Euclidean distance), but
    other $p$-norms can be chosen. This likely depends on the real-world meaning of the data.

    One other important note on nearest-neighbor schemes is the importance of
    standardizing the data before running the algorithm. Because the distance between
    data points is used, if one feature is of a larger scale, it can easily drown out others, and
    if it is of a smaller scale, it can easily be drowned out by others.

    \subsection*{Edited KNN}
    The motivation behind Edited KNN is twofold. Consider that with the standard KNN, at
    prediction time, given $n$ training points, $n$ distance calculations must be made in order
    to make a single prediction. If the set of training points can be edited such that there are
    fewer points, but the predictions are roughly the same, prediction becomes computationally easier.
    Additionally, if there are many points in the training set that are not near their class
    centers, prediction quality may suffer, as an instance near them may be misclassified.

    Edited KNNs\cite{edited_knn} are very similar to the standard KNN. At fitting time, each of the
    training points $x_i$ are iteratively classified using all of the other data points, $X_{-i}$.
    If the prediction $\hat{y}_i \neq y_i$, then $x_i$ is dropped from the training set. In \cite{edited_knn},
    all points are considered for removal once. Alternatively, this process can be
    repeated until no further changes are made, or until performance on a validation set
    starts to decrease.

    Alternatively, the editing procedure can remove points that are correctly classified, with the
    hypothesis that they are likely not needed for correct classification going forward.

    \subsection*{Condensed KNN}
    The motivation for the Condensed KNN is similar to that of the Edited KNN. It is desirable
    to have fewer points in the training set, and to retain only those points that are needed
    to avoid an increase in error.

    The condensing algorithm begins with an empty set $Z = \emptyset$. The first data point
    considered is added to $Z$. Then, for each of the remaining data points $x_i$,
    the distances between $x_i$ and all points in $Z$ are calculated, and the nearest data point $z_*$
    is found. If the class of $x_i$ is not equal to the class of $z_*$,
    $x_i$ is added to $Z$. This process is repeated until no changes are made.

    Note that at prediction time, the class of the single nearest neighbor is used.

    \section{Description of Experimental Approach}
    The 3 algorithms were applied to 2 real life classification problems:
    \begin{enumerate}
        \item The Ecoli dataset for classifying the localization site of protein.
        \item The Image Segmentation dataset for classifying the part of the image of a given pixel.
    \end{enumerate}
    Standard KNN was applied to 2 additional regression problems.
    \begin{enumerate}
        \item The CPU Performance dataset for predicting the performance of a given CPU (regression problem).
        \item The Forest Fires dataset in which the area burned is predicted (regression problem).
    \end{enumerate}

    Discrete inputs were one-hot encoded. A 5-Fold cross validation scheme was used to estimate the expected out-of-sample
    error. For classification problems, a stratefied cross-validation approach was used to accurately
    represent the various classes. Within the cross-validation scheme, $k$ was tuned by choosing
    \[
        \argmax_{1 \leq k \leq 5} f(k),
    \]
    where $f(k)$ was accuracy for classification problems, and negative RMSE for regression problems. $k$ was chosen
    by running 5-fold CV over the first training set of the overall 5-fold CV, and taking the mean of $f(k)$ across the
    folds.

    Each dataset was standardized dividing by the column-wise standard deviation. This
    ensures that the scale of each feature is the same (a distance of 1 indicates a 1
    standard deviation difference).

    The KNN structure naturally handles multi-class classification problems, and regression
    problems were solved by taking the mean across the target values of the $k$ neighbors.

    \section{Experimental Results}
    \subsection*{Ecoli Data}
    The first experimental dataset is the Ecoli data\cite{ecoli_dataset},
    a classification problem. All 3 algorithms above are applied,
    and the results are shown in Table~\ref{ecoli_results}.

    \begin{table}[H]
    \begin{tabular}{lcc}
        Algorithm     & $k$ & Accuracy \\
        \hline
        Standard KNN  & 4 & 81.9\% \\
        Edited KNN    & 2 & 77.3\% \\
        Condensed KNN & 1 & 78.6\% \\
    \end{tabular}
    \label{ecoli_results}
    \caption{The results of various KNN variants on the Ecoli dataset for classification.}
    \end{table}
    As noted above, $k$ was chosen using cross-validation on the training set of the first of the 5-folds. The highest
    accuracy across the cross-validation folds (within the first fold) was selected. $k$ was set to one for the
    condensed algorithm.

    \subsection*{Image Segmentation}
    The second experimental dataset is the Image Segmentation dataset\cite{image_dataset}, a classification problem. The goal is to
    classify pixel to the portion of the image from which they came. All 3 algorithms above are applied,
    and the results are shown in Table~\ref{image_segmentation_results}. Note that the "REGION-PIXEL-COUNT"
    class is dropped, as it does not have any variance.
    \begin{table}[H]
    \begin{tabular}{lcc}
        Algorithm     & $k$ & Accuracy \\
        \hline
        Standard KNN  & 5 & 85.2\% \\
        Edited KNN    & 1 & 66.2\% \\
        Condensed KNN & 1 & 81.42\% \\
    \end{tabular}
    \label{image_segmentation_results}
    \caption{The results of various KNN variants on the Image Segmentation dataset for classification.}
    \end{table}
    As noted above, $k$ was chosen using cross-validation on the training set of the first of the 5-folds. The highest
    accuracy across the cross-validation folds (within the first fold) was selected. $k$ was set to one for the
    condensed algorithm.

    \subsection*{CPU Performance}
    The third experimental dataset is the CPU performance dataset\cite{cpu_dataset}. The goal is to predict the relative performance
    given a set of characteristics of a CPU. Only the standard KNN is applied to the regression problem, as the edited
    and condensed algorithms rely on class labels. The results are shown in Table~\ref{cpu_performance_results}.
    \begin{table}[H]
    \begin{tabular}{lcc}
        Algorithm     & $k$ & RMSE \\
        \hline
        Standard KNN  & 1 & 63.4
    \end{tabular}
    \label{cpu_performance_results}
    \caption{The results of a KNN on the CPU performance regression problem.}
    \end{table}

    \subsection*{Forest Fires}
    The fourth experimental dataset is a regression problem in which the area of a given
    forest fire is predicted based on features about the time and weather of the fire\cite{ff_dataset}.
    In preprocessing the data, features describing the month and day are one-hot encoded.
    The results of the experiments are shown in Table~\ref{fire_forest_area}.
    As noted in \cite{ff_dataset}, this is a very difficult regression problem, and the performance
    of the KNN is well below that of the naive mean prediction.
    \begin{table}[H]
    \begin{tabular}{lcc}
        Algorithm     & $k$ & RMSE \\
        \hline
        Standard KNN  & 4 & 91.8
    \end{tabular}
    \label{fire_forest_area}
    \caption{The results of a KNN on the Forest Fire Area regression problem.}
    \end{table}

    \section{Algorithm Behavior and Conclusions}
    The performance of the algorithms on classification problems was not unexpected,
    as the standard KNN outperformed the edited and condensed algorithms. The hypothesis
    was that the edited and condensed versions might outperform if the data were noisy or if
    irrelevant features were included, but absent that, the larger samples should provide better
    estimation.

    On the regression problems, the KNN did not perform particularly well. For the
    CPU performance dataset, the linear regression model cited\cite{cpu_dataset}
    outperformed the KNN, although the KNN outperformed the naive mean prediction model.
    On the Forest Fires dataset\cite{ff_dataset}, the KNN did not outperform a linear regression
    model fit on the same set of features, nor did either of them outperform the
    naive mean prediction model. This was not unexpected, as the authors referenced the
    difficulty of the problem, and noted that in terms of RMSE, none of the battery of
    learning methods applied outperformed the naive mean prediction model.

    It is difficult to draw generalizations about the performance of learning
    algorithms from only a few datasets, but one structural limitation is worth
    mentioning. KNNs struggle in high dimensional spaces. This was evidenced by the forest fires
    data, which has a feature space in $\mathbb{R}^{27}$, once the categorical
    variables are one-hot encoded. This is exacerbated by the inclusion of features that
    are irrelevant, as the algorithm has little mechanism for determine
    what is and what is not important.

    Additionally, if parametric forms are appropriate for a problem, they may outperform
    KNN - this occurs in the forest fires regression problem, where some transformations
    and a linear regression model outperform the KNN. This may contain a valuable
    detail about the KNN - it should be applied in cases where the parametric form is unknown (e.g.
    not easily linearly seperable), and the flexible Voronoi diagram created by KNN is a desirable quality.

    \section{Summary}
    This paper introduced the K Nearest-Neighbors algorithm for non-parametric supervised learning,
    as well as two variants of the classification algorithm designed for efficient inference, and
    potentially improved performance, called Edited and Condensed KNN. The algorithms select
    training points that are close in feature space to make predictions about the inference point.

    The KNN was applied to 4 problems, two classification and two regression, while the variants
    are only applicable to classification, and therefore were only applied to those problems.

    It was found that KNN outperforms its variants on the classification problems, and performs inline
    with other learning algorithms on the regression problems.


\bibliographystyle{plainurl}
\bibliography{biblio}

\end{document}